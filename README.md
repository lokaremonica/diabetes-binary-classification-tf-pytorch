# ğŸ§  Diabetes Prediction using Deep Neural Networks (TensorFlow & PyTorch)

This project implements a binary classification system to predict the likelihood of diabetes using feedforward neural networks in both **TensorFlow** and **PyTorch**. The model is trained on a medical dataset with 8 normalized features and one binary outcome.

Thanks, Monica! Iâ€™ve thoroughly reviewed your `Assignment1.pdf`, which includes well-documented TensorFlow and PyTorch implementations of binary classification using a feedforward neural network on the diabetes dataset.

---

## ğŸ“Š Dataset Overview

* The dataset consists of **759 rows** and **9 columns** (8 features + 1 binary target).
* It includes anonymized, normalized patient data.

| Column     | Description                            |
| ---------- | -------------------------------------- |
| Feature1â€“8 | Numeric features                       |
| Feature9   | Target (0 = No Diabetes, 1 = Diabetes) |

---

## ğŸ” Problem Statement

Predict whether a patient has diabetes based on health-related features using feedforward neural networks.

---

## âš™ï¸ Technologies Used

| Framework            | Purpose                             |
| -------------------- | ----------------------------------- |
| TensorFlow           | Neural network modeling (Keras API) |
| PyTorch              | Neural network modeling (nn.Module) |
| Pandas / NumPy       | Data manipulation                   |
| Seaborn / Matplotlib | Visualization                       |
| Scikit-learn         | Data splitting, metrics             |

---

## ğŸ”¬ Workflow Summary

### âœ… Data Preparation

* Normalized numeric features
* Checked for missing values
* Train-test split: 70% / 30%

### ğŸ“Š Exploratory Data Analysis

* Histograms of features
* Correlation heatmap
* Class distribution bar plot

---

## ğŸ§  Model Architectures

### ğŸ”¹ Base Model (both TF & PyTorch)

```
Input (8 features) â†’ Dense(32, ReLU) â†’ Dense(16, ReLU) â†’ Dense(1, Sigmoid)
```

### ğŸ”¹ Hypothesis 1: Wider Network

```
Input â†’ Dense(64) â†’ Dense(32) â†’ Output
```

### ğŸ”¹ Hypothesis 2: Deeper + Wider Network

```
Input â†’ Dense(64) â†’ Dense(32) â†’ Dense(16) â†’ Output
```

---

## ğŸ“ˆ Results Summary

| Model Type           | Accuracy | F1-Score (Class 1) |
| -------------------- | -------- | ------------------ |
| Original TF Model    | 0.7368   | 0.80               |
| Wider TF Model       | 0.7588   | 0.82               |
| Deeper TF Model      | 0.7588   | 0.82               |
| Original PyTorch     | 0.7368   | 0.80               |
| Wider PyTorch Model  | 0.7544   | 0.82               |
| Deeper PyTorch Model | 0.7675   | 0.81               |

> ğŸ” Conclusion: Increased width/depth improves performance slightly. Deeper PyTorch model achieved the best results.

---

## ğŸ“Š Visualizations

* Accuracy and loss over epochs
* Model comparison plots
* Classification reports

---

## ğŸ“Œ Key Learnings

* **Width** improves feature learning and helps shallow networks generalize better.
* **Depth** improves hierarchical pattern learning but may cause vanishing gradients if not managed properly.
* Both frameworks (TF and PyTorch) achieved similar performance with comparable architectures.

---


